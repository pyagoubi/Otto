{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport ast\nimport json\nimport glob\nimport torch\nimport warnings\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport gc\n#import cudf\nimport joblib\n\nfrom tqdm import tqdm\nfrom datetime import datetime\nfrom collections import Counter\nfrom sklearn import preprocessing\nfrom tqdm import tqdm\n\n                        \nfrom torch.utils.data import DataLoader\nimport pytorch_lightning as pl\nfrom pytorch_lightning.loggers import TensorBoardLogger\nfrom pytorch_lightning.callbacks import ModelCheckpoint\nimport torch\nimport torch.nn as nn\nfrom torch.nn import Linear\nfrom torch.nn import functional as F\n\n\nimport gensim\n#from gensim.test.utils import common_texts\nfrom gensim.models import Word2Vec","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_w1 = dict() #training first week, validation 4th week\nlabel_w1 = dict()\n\n\ntrain_w4 = dict() #val_dataset\nlabel_w4= dict()  #val_dataset\n\nfor file in glob.glob(\"/kaggle/input/otto-prep-4-weeks/train_w0_part*.parquet\"):\n   train_w1[file] = pd.read_parquet(file)\ntrain_w1 = pd.concat(train_w1.values())\n\nfor file in glob.glob(\"/kaggle/input/otto-prep-4-weeks/label_w0_part*.parquet\"):\n   label_w1[file] = pd.read_parquet(file)\nlabel_w1 = pd.concat(label_w1.values())\n\nfor file in glob.glob(\"/kaggle/input/otto-training-wo-split/train_w3_part*.parquet\"):\n   train_w4[file] = pd.read_parquet(file)\ntrain_w4 = pd.concat(train_w4.values())\n\nfor file in glob.glob(\"/kaggle/input/otto-prep-4-weeks/label_w3_part*.parquet\"):\n   label_w4[file] = pd.read_parquet(file)\nlabel_w4 = pd.concat(label_w4.values())\n\n\n#max aid unique 486","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# discard weeks 2-3 for simplicity\n\n#train_w2 = dict()\n#label_w2 = dict()\n\n# train_w3 = dict() \n# label_w3 = dict()\n\n\n# for file in glob.glob(\"/kaggle/input/otto-training-wo-split/train_w1_part*.parquet\"):\n#    train_w2[file] = pd.read_parquet(file)\n# train_w2 = pd.concat(train_w2.values())\n\n# for file in glob.glob(\"/kaggle/input/otto-prep-4-weeks/label_w1_part*.parquet\"):\n#    label_w2[file] = pd.read_parquet(file)\n# label_w2 = pd.concat(label_w2.values())\n\n# for file in glob.glob(\"/kaggle/input/otto-training-wo-split/train_w2_part*.parquet\"):\n#    train_w3[file] = pd.read_parquet(file)\n# train_w3 = pd.concat(train_w3.values())\n\n# for file in glob.glob(\"/kaggle/input/otto-prep-4-weeks/label_w2_part*.parquet\"):\n#    label_w3[file] = pd.read_parquet(file)\n# label_w3 = pd.concat(label_w3.values())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def map_column(df: pd.DataFrame, col_name: str):\n    all_aid = pd.read_parquet('/kaggle/input/otto-full-optimized-memory-footprint/train.parquet')\n    aid_sorted = sorted(list(all_aid[col_name].unique()))\n    del all_aid\n    \n    mapping = {k: i + 2 for i, k in enumerate(aid_sorted)}\n    inverse_mapping = {v: k for k, v in mapping.items()}\n\n    df[col_name] = df[col_name].map(mapping)\n\n    return df, mapping, inverse_mapping","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_w1,mapping, inverse_mapping = map_column(train_w1, 'aid')\n\ntrain_w4['aid'] = train_w4['aid'].map(mapping)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_w4 = train_w4[~train_w4.session.isin(train_w4[train_w4['aid'].isna()]['session'].unique())]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_merged(df, label_df):\n\n    df = pd.DataFrame(df.groupby('session')['aid'].unique().agg(list))\n    label_df = pd.DataFrame(label_df.groupby('session')['aid'].unique().agg(list))\n    \n    df.rename(columns = {'aid': 'input'}, inplace = True)\n    label_df.rename(columns = {'aid': 'label'}, inplace = True)\n\n    df = df.reset_index()\n    label_df = label_df.reset_index()\n\n    df = pd.merge(df, label_df, on = \"session\", how = \"inner\")  \n    \n    return df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_w1 = get_merged(train_w1, label_w1)\ntrain_w4 = get_merged(train_w4, label_w4)\ndel label_w1, label_w4","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Load pre-trained Word2Vec model.\nw2v = gensim.models.Word2Vec.load(\"/kaggle/input/otto-w2vec/word2vec.model\")\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"EMBEDDING_LENGTH = 32\nTARGET_LENGTH = 20\nSEQUENCE_LENGTH = 100\n\nclass Dataset(torch.utils.data.Dataset):\n    def __init__(self, sessions, input_length = SEQUENCE_LENGTH, target_length = TARGET_LENGTH):\n        self.sessions = sessions\n        self.input_length = input_length\n        self.target_length = target_length\n\n    def __len__(self):\n        return len(self.sessions)\n    \n    def pad_items(self, session, length, input_item = True):\n        if len(session)< length:\n            session = session + list((length - len(session)) * [0])\n        else: \n            if input_item:\n                session = session[-length:]\n            else: session = session[:length]\n        return session\n\n\n\n    def __getitem__(self, idx):\n        input_tokens = self.sessions.iloc[idx, self.sessions.columns.get_loc(\"input\")]\n        \n        length_input = len(input_tokens)\n        input_tokens = self.pad_items(input_tokens, self.input_length)\n            \n        target = self.sessions.iloc[idx, self.sessions.columns.get_loc(\"label\")]\n        target = self.pad_items(target, self.target_length, input_item = False)\n        \n        target_mask = [1 if item != 0 else item for item in target ]\n        \n        \n        target = input_tokens +target\n        input_tokens = input_tokens + target_mask \n        \n        \n        \n        input_token_orig = torch.tensor(input_tokens)\n        input_tokens = w2v.wv[input_tokens]\n        \n        input_tokens = torch.tensor(input_tokens, dtype = torch.float )\n        target = torch.tensor(target, dtype = torch.long )\n\n        \n        \n        mask = torch.eq(input_token_orig, 0).type(torch.bool).unsqueeze(0).permute(1,0)\n        \n       \n        return input_tokens, target, mask\n                                     ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Recommender(pl.LightningModule):\n    def __init__(\n        self,\n        out = len(mapping)+2,\n        channels=EMBEDDING_LENGTH,\n        dropout=0.2,\n        lr=1e-4,\n        word2vec = w2v\n    ):\n        super().__init__()\n        \n        self.lr = lr\n        self.dropout = dropout\n        self.out = out\n        \n        self.item_embeddings = w2v\n\n        encoder_layer = nn.TransformerEncoderLayer(\n            d_model=channels, nhead=4, dropout=self.dropout, batch_first = True\n        )\n\n        self.encoder = torch.nn.TransformerEncoder(encoder_layer, num_layers=10)\n\n        self.linear_out = Linear(channels, self.out)\n\n        self.do = nn.Dropout(p=self.dropout)\n\n\n    def forward(self, input_items, mask):\n\n        mask = mask.squeeze(-1)\n        \n        if torch.cuda.is_available():\n            input_items = input_items.to(\"cuda:0\")\n            mask = mask.to(\"cuda:0\")\n        \n        \n        input_items = self.encoder(input_items, src_key_padding_mask =mask)\n        \n        out = self.linear_out(input_items)\n\n        return out\n\n    def training_step(self, batch, batch_idx):\n        \n        \n        input_items, y_true, mask = batch\n\n        y_pred = self(input_items, mask)\n        \n\n        if torch.cuda.is_available():\n            y_pred = y_pred.to(\"cuda:0\")\n            y_true = y_true.to(\"cuda:0\")\n        \n        \n        y_pred = y_pred.view(-1, y_pred.size(2))\n        y_true = y_true.view(-1)\n        \n        print('pred',y_pred.shape)\n        print('tr',y_true.shape)\n        \n        \n        input_items = input_items.view(-1)\n        loss_mask = input_items == 1\n        \n        loss = F.cross_entropy(y_pred, y_true, reduction=\"none\")\n        \n        loss = loss * loss_mask\n\n        loss = loss.sum() / (loss_mask.sum() + 1e-8)    \n        \n        \n        \n        accuracy = (y_true == y_pred).double().mean()\n\n\n        self.log(\"train_loss\", loss, prog_bar=True, on_step=False, on_epoch=True)\n        self.log(\"train_accuracy\", accuracy)\n\n        return loss   \n    \n    \n\n    def validation_step(self, batch, batch_idx):\n        input_items, y_true, mask = batch\n\n        y_pred = self(input_items, mask)\n\n        y_pred = y_pred.view(-1, y_pred.size(2))\n        y_true = y_true.view(-1)\n\n        input_items = input_items.view(-1)\n        loss_mask = input_items == 1\n        \n        loss = F.cross_entropy(y_pred, y_true, reduction=\"none\")\n        \n        loss = loss * loss_mask\n\n        loss = loss.sum() / (loss_mask.sum() + 1e-8)    \n        \n\n        self.log('valid_loss', loss, prog_bar=True, on_step=False, on_epoch=True)\n        self.log(\"valid_accuracy\", accuracy)\n\n        return loss\n\n\n    def configure_optimizers(self):\n        optimizer = torch.optim.Adam(self.parameters(), lr=self.lr)\n        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n            optimizer, patience=10, factor=0.1\n        )\n        return {\n            \"optimizer\": optimizer,\n            \"lr_scheduler\": scheduler,\n            \"monitor\": \"valid_loss\",\n        }\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"batch_size = 5\n\n\ntrain_data = Dataset(\n    sessions=train_w1\n)\n\nval_data = Dataset(\nsessions = train_w4\n)\n\nprint(\"len(train_data)\", len(train_data))\nprint(\"len(val_data)\", len(val_data))\n\n\ntrain_loader = DataLoader(\n    train_data,\n    batch_size=batch_size,\n    num_workers=2,\n    shuffle=True,\n)\nval_loader = DataLoader(\n    val_data,\n    batch_size=batch_size,\n    num_workers=2,\n    shuffle=False,\n)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"epochs = 10\n\nmodel = Recommender()\ntrainer = pl.Trainer(\n    max_epochs=epochs,\n    gpus=1\n)\n\nlogger = TensorBoardLogger(\n    save_dir='/kaggle/working/',\n    )\n\ncheckpoint_callback = ModelCheckpoint(\n    monitor=\"valid_loss\",\n    mode=\"min\",\n    dirpath='/kaggle/working/',\n    filename=\"recommender\",\n    )\n\n\n\ntrainer.fit(model, train_loader, val_loader)\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gc.collect()","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}