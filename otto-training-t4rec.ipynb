{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport ast\nimport json\nimport glob\nimport torch\nimport warnings\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport gc\n#import cudf\nimport joblib\n\nfrom tqdm import tqdm\nfrom datetime import datetime\nfrom collections import Counter\nfrom sklearn import preprocessing\nfrom tqdm import tqdm\n\n                        \n\nimport pytorch_lightning as pl\nimport torch\nimport torch.nn as nn\nfrom torch.nn import Linear\nfrom torch.nn import functional as F\n\n\nimport gensim\nfrom gensim.test.utils import common_texts\nfrom gensim.models import Word2Vec","metadata":{"execution":{"iopub.status.busy":"2022-12-22T17:13:43.210897Z","iopub.execute_input":"2022-12-22T17:13:43.211783Z","iopub.status.idle":"2022-12-22T17:13:48.162937Z","shell.execute_reply.started":"2022-12-22T17:13:43.211669Z","shell.execute_reply":"2022-12-22T17:13:48.161950Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"train_w1 = dict() #train data, other weeks will be added later\nlabel_w1 = dict()\n\n#train_w2 = dict()\n#label_w2 = dict()\n\n# train_w3 = dict() \n# label_w3 = dict()\n\ntrain_w4 = dict() #val_dataset\nlabel_w4= dict()  #val_dataset\n\nfor file in glob.glob(\"/kaggle/input/otto-prep-4-weeks/train_w0_part*.parquet\"):\n   train_w1[file] = pd.read_parquet(file)\ntrain_w1 = pd.concat(train_w1.values())\n\nfor file in glob.glob(\"/kaggle/input/otto-prep-4-weeks/label_w0_part*.parquet\"):\n   label_w1[file] = pd.read_parquet(file)\nlabel_w1 = pd.concat(label_w1.values())\n\n# for file in glob.glob(\"/kaggle/input/otto-training-wo-split/train_w1_part*.parquet\"):\n#    train_w2[file] = pd.read_parquet(file)\n# train_w2 = pd.concat(train_w2.values())\n\n# for file in glob.glob(\"/kaggle/input/otto-prep-4-weeks/label_w1_part*.parquet\"):\n#    label_w2[file] = pd.read_parquet(file)\n# label_w2 = pd.concat(label_w2.values())\n\n# for file in glob.glob(\"/kaggle/input/otto-training-wo-split/train_w2_part*.parquet\"):\n#    train_w3[file] = pd.read_parquet(file)\n# train_w3 = pd.concat(train_w3.values())\n\n# for file in glob.glob(\"/kaggle/input/otto-prep-4-weeks/label_w2_part*.parquet\"):\n#    label_w3[file] = pd.read_parquet(file)\n# label_w3 = pd.concat(label_w3.values())\n\nfor file in glob.glob(\"/kaggle/input/otto-training-wo-split/train_w3_part*.parquet\"):\n   train_w4[file] = pd.read_parquet(file)\ntrain_w4 = pd.concat(train_w4.values())\n\nfor file in glob.glob(\"/kaggle/input/otto-prep-4-weeks/label_w3_part*.parquet\"):\n   label_w4[file] = pd.read_parquet(file)\nlabel_w4 = pd.concat(label_w4.values())\n\n\n#max aid unique 486","metadata":{"execution":{"iopub.status.busy":"2022-12-22T17:13:48.165617Z","iopub.execute_input":"2022-12-22T17:13:48.167456Z","iopub.status.idle":"2022-12-22T17:14:09.284873Z","shell.execute_reply.started":"2022-12-22T17:13:48.167384Z","shell.execute_reply":"2022-12-22T17:14:09.283801Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"def label_encoding(df):\n\n    #set start = 2 to reserve 0 for mask and 1 for pad\n    df['aid+2'] = df['aid']+2\n    df['aid'] = df['aid+2']\n\n    df = df.drop(['aid+2'], axis = 1)\n    return df","metadata":{"execution":{"iopub.status.busy":"2022-12-22T17:14:09.286206Z","iopub.execute_input":"2022-12-22T17:14:09.286661Z","iopub.status.idle":"2022-12-22T17:14:09.292721Z","shell.execute_reply.started":"2022-12-22T17:14:09.286619Z","shell.execute_reply":"2022-12-22T17:14:09.291519Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"train_w1 = label_encoding(train_w1)\ntrain_w4 = label_encoding(train_w4)","metadata":{"execution":{"iopub.status.busy":"2022-12-22T17:14:09.296209Z","iopub.execute_input":"2022-12-22T17:14:09.296669Z","iopub.status.idle":"2022-12-22T17:14:12.597699Z","shell.execute_reply.started":"2022-12-22T17:14:09.296625Z","shell.execute_reply":"2022-12-22T17:14:12.596707Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"def get_merged(df, label_df):\n\n    df = pd.DataFrame(df.groupby('session')['aid'].unique().agg(list))\n    label_df = pd.DataFrame(label_df.groupby('session')['aid'].unique().agg(list))\n    \n    df.rename(columns = {'aid': 'input'}, inplace = True)\n    label_df.rename(columns = {'aid': 'label'}, inplace = True)\n\n    df = df.reset_index()\n    label_df = label_df.reset_index()\n\n    df = pd.merge(df, label_df, on = \"session\", how = \"inner\")  \n    \n    return df","metadata":{"execution":{"iopub.status.busy":"2022-12-22T17:14:12.599173Z","iopub.execute_input":"2022-12-22T17:14:12.599549Z","iopub.status.idle":"2022-12-22T17:14:12.608925Z","shell.execute_reply.started":"2022-12-22T17:14:12.599515Z","shell.execute_reply":"2022-12-22T17:14:12.607860Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"train_w1 = get_merged(train_w1, label_w1)\ntrain_w4 = get_merged(train_w4, label_w4)\ndel label_w1, label_w4","metadata":{"execution":{"iopub.status.busy":"2022-12-22T17:14:12.610323Z","iopub.execute_input":"2022-12-22T17:14:12.610742Z","iopub.status.idle":"2022-12-22T17:28:42.039635Z","shell.execute_reply.started":"2022-12-22T17:14:12.610713Z","shell.execute_reply":"2022-12-22T17:28:42.038702Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"# Load pre-trained Word2Vec model.\nw2v = gensim.models.Word2Vec.load(\"/kaggle/input/otto-w2vec/word2vec.model\")\n\n","metadata":{"execution":{"iopub.status.busy":"2022-12-22T17:28:42.041173Z","iopub.execute_input":"2022-12-22T17:28:42.041626Z","iopub.status.idle":"2022-12-22T17:29:05.868877Z","shell.execute_reply.started":"2022-12-22T17:28:42.041586Z","shell.execute_reply":"2022-12-22T17:29:05.867929Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"INPUT_LENGTH = 100\nOUTPUT_LENGTH = 20","metadata":{"execution":{"iopub.status.busy":"2022-12-22T17:29:05.870147Z","iopub.execute_input":"2022-12-22T17:29:05.870457Z","iopub.status.idle":"2022-12-22T17:29:05.875688Z","shell.execute_reply.started":"2022-12-22T17:29:05.870429Z","shell.execute_reply":"2022-12-22T17:29:05.874662Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"class Dataset(torch.utils.data.Dataset):\n    def __init__(self, sessions, input_length = INPUT_LENGTH, output_length = OUTPUT_LENGTH):\n        self.sessions = sessions\n        self.input_length = input_length\n        self.output_length = output_length\n\n    def __len__(self):\n        return len(self.sessions)\n    \n    def pad_items(self, session, length):\n        if len(session)< length:\n            session = session + list(length - len(session) * [0])\n        else: session = session[-length:]\n        return session\n      \n    def __getitem__(self, idx):\n        input_tokens = sessions.iloc[idx, sessions.columns.get_loc(\"input\")]\n        \n        length_input = len(input_tokens)\n        input_tokens = pad_items(input, self.input_length) \n        input_tokens = w2v.wv[input_tokens]\n        \n        target = sessions.iloc[idx, sessions.columns.get_loc(\"label\")]\n        target = pad_items(target, self.output_length) \n        \n        input_tokens = torch.tensor(input_tokens, dtype=torch.long)\n        target = torch.tensor(target, dtype=torch.long)\n        \n        mask = torch.tensor(list([1]*length_input + list(self.input_length * [0])))\n        \n        return input_tokens, target, mask\n                                     ","metadata":{"execution":{"iopub.status.busy":"2022-12-22T17:29:05.877261Z","iopub.execute_input":"2022-12-22T17:29:05.877734Z","iopub.status.idle":"2022-12-22T17:29:05.907192Z","shell.execute_reply.started":"2022-12-22T17:29:05.877696Z","shell.execute_reply":"2022-12-22T17:29:05.906104Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"class Recommender(pl.LightningModule):\n    def __init__(\n        self,\n        out = OUTPUT_LENGTH,\n        channels=INPUT_LENGTH,\n        dropout=0.2,\n        lr=1e-4,\n        word2vec = w2v\n    ):\n        super().__init__()\n\n        self.item_embeddings = w2v\n\n        encoder_layer = nn.TransformerEncoderLayer(\n            d_model=channels, nhead=4, dropout=self.dropout\n        )\n\n        self.encoder = torch.nn.TransformerEncoder(encoder_layer, num_layers=10, src_mask= mask)\n\n        self.linear_out = Linear(channels, self.out)\n\n        self.do = nn.Dropout(p=self.dropout)\n\n\n    def forward(self, input_items):\n\n        input_items = self.encoder(input_items, mask)\n\n        out = self.linear_out(input_items)\n\n        return out\n\n    def training_step(self, batch, batch_idx):\n        input_items, y_true, mask = batch\n\n        y_pred = self(input_items, mask)\n\n        loss = F.cross_entropy(y_pred, y_true, reduction=\"none\")\n        accuracy = (y_true == y_pred).double().mean()\n\n        self.log(\"train_loss\", loss)\n        self.log(\"train_accuracy\", accuracy)\n\n        return loss\n\n    def validation_step(self, batch, batch_idx):\n        input_items, y_true = batch\n\n        y_pred = self(input_items)\n\n        loss = F.cross_entropy(y_pred, y_true, reduction=\"none\")\n        accuracy = (y_true == y_pred).double().mean()\n\n        self.log(\"valid_loss\", loss)\n        self.log(\"valid_accuracy\", accuracy)\n\n        return loss\n\n\n    def configure_optimizers(self):\n        optimizer = torch.optim.Adam(self.parameters(), lr=self.lr)\n        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n            optimizer, patience=10, factor=0.1\n        )\n        return {\n            \"optimizer\": optimizer,\n            \"lr_scheduler\": scheduler,\n            \"monitor\": \"valid_loss\",\n        }","metadata":{"execution":{"iopub.status.busy":"2022-12-22T17:29:05.911051Z","iopub.execute_input":"2022-12-22T17:29:05.911380Z","iopub.status.idle":"2022-12-22T17:29:05.925370Z","shell.execute_reply.started":"2022-12-22T17:29:05.911351Z","shell.execute_reply":"2022-12-22T17:29:05.924469Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"\ntrain_data = Dataset(\n    sessions=train_w1\n)\n\nval_data = Dataset(\nsessions = train_w4\n)\n\nprint(\"len(train_data)\", len(train_data))\nprint(\"len(val_data)\", len(val_data))\n","metadata":{"execution":{"iopub.status.busy":"2022-12-22T17:29:05.926542Z","iopub.execute_input":"2022-12-22T17:29:05.926833Z","iopub.status.idle":"2022-12-22T17:29:05.942347Z","shell.execute_reply.started":"2022-12-22T17:29:05.926808Z","shell.execute_reply":"2022-12-22T17:29:05.941259Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"len(train_data) 4441142\nlen(val_data) 4356191\n","output_type":"stream"}]},{"cell_type":"code","source":"epochs = 10\n\nmodel = Recommender()\ntrainer = pl.Trainer(\n    max_epochs=epochs,\n    gpus=1\n)\n\ntrainer.fit(model, train_loader, val_loader)","metadata":{},"execution_count":null,"outputs":[]}]}